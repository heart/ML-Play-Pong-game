import game
import tensorflow as tf
import numpy as np
import random
import time
import os
from collections import deque

# ‚úÖ ‡∏õ‡∏¥‡∏î Log ‡∏Ç‡∏≠‡∏á TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel('ERROR')

# ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ GPU ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("‚úÖ ‡πÉ‡∏ä‡πâ GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Training!")
else:
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU, ‡πÉ‡∏ä‡πâ CPU ‡πÅ‡∏ó‡∏ô")

# **‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á RL**
gamma = 0.95
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.999
learning_rate = 0.001
batch_size = 32
max_memory_size = 10000
tau = 0.01  # Soft Update Rate
memory = deque(maxlen=max_memory_size)
scores = []
num_frames = 4  # ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á

state_size = (game.sceneSize, game.sceneSize, num_frames)
action_size = 3

# **‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ**
latest_model = None
latest_episode = 0
for i in range(100000, 0, -1000):
    filename = f"ai_pong_{i}.h5"
    if os.path.exists(filename):
        latest_model = filename
        latest_episode = i
        break

if latest_model:
    print(f"üîÑ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å {latest_model} (‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß {latest_episode} episodes)")
    model = tf.keras.models.load_model(latest_model)
    epsilon = max(epsilon_min, epsilon * (epsilon_decay ** latest_episode))
else:
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏´‡∏°‡πà ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà Episode 0")
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=state_size),
        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(action_size, activation='linear')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), 
                  loss='mse', 
                  jit_compile=True)

# **‡∏™‡∏£‡πâ‡∏≤‡∏á Target Network**
target_model = tf.keras.models.clone_model(model)
target_model.set_weights(model.get_weights())

# **‡∏™‡∏£‡πâ‡∏≤‡∏á Stack ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ü‡∏£‡∏°‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á**
frame_stack = deque(maxlen=num_frames)

def reset_stacked_state():
    """ ‚úÖ ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï Frame Stack ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Å‡∏°‡πÉ‡∏´‡∏°‡πà """
    global frame_stack
    frame_stack.clear()

def get_stacked_state():
    """ ‚úÖ ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ state ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏ü‡∏£‡∏°‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á num_frames ‡πÄ‡∏ü‡∏£‡∏° """
    state, ballXSpeed, ballYSpeed = game.get_state()
    state = np.expand_dims(state, axis=-1).astype(np.float32) / 2  # Normalize

    if len(frame_stack) < num_frames:
        for _ in range(num_frames):
            frame_stack.append(state)

    frame_stack.append(state)
    stacked_state = np.concatenate(frame_stack, axis=-1)
    return stacked_state, ballXSpeed, ballYSpeed

def get_action(state):
    if np.random.rand() <= epsilon:
        return random.choice([0, 1, 2])
    return np.argmax(model.predict(state.reshape(1, game.sceneSize, game.sceneSize, num_frames), verbose=0))

def replay():
    """ ‚úÖ ‡πÉ‡∏ä‡πâ Mini-Batch Training """
    if len(memory) < batch_size:
        return

    batch = random.sample(memory, batch_size)
    
    states = np.array([sample[0] for sample in batch])
    actions = np.array([sample[1] for sample in batch])
    rewards = np.array([sample[2] for sample in batch])
    next_states = np.array([sample[3] for sample in batch])
    dones = np.array([sample[4] for sample in batch])

    targets = model.predict(states, verbose=0)
    next_q_values = target_model.predict(next_states, verbose=0)

    for i in range(batch_size):
        target = rewards[i]
        if not dones[i]:
            target += gamma * np.max(next_q_values[i])
        targets[i][actions[i]] = target

    model.fit(states, targets, batch_size=batch_size, epochs=1, verbose=0)

def train_dqn(episodes=5000):
    global epsilon
    for episode in range(latest_episode + 1, latest_episode + episodes + 1):
        reset_stacked_state()  
        state, ballXSpeed, ballYSpeed = get_stacked_state()
        done = False
        total_reward = 0

        while not done:
            action = get_action(state)

            if action == 0:
                game.move_left()
            elif action == 2:
                game.move_right()

            hit, game_over = game.tick()
            next_state, next_ballXSpeed, next_ballYSpeed = get_stacked_state()

            # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Reward ‡πÉ‡∏´‡πâ AI
            reward = 1  # ‡∏õ‡∏Å‡∏ï‡∏¥
            if hit:
                reward = 10  # ‡∏ï‡∏µ‡πÇ‡∏î‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πâ‡∏°
            if game_over:
                reward = -100  # ‡πÅ‡∏û‡πâ‡πÄ‡∏Å‡∏°
                done = True

            total_reward += reward  # ‚úÖ AI ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•

            memory.append((state, action, reward, next_state, done))
            if len(memory) >= batch_size:
                replay()  # ‚úÖ Train AI

            state = next_state

            game.render()
            print(f"Episode {episode}: Score = {total_reward}, Epsilon = {epsilon:.4f}")
            
            if game_over:
                game.reset()
                reset_stacked_state()

        scores.append(total_reward)  # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô

        # ‚úÖ ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ñ‡πâ‡∏≤ AI ‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ
        if len(scores) > 100 and np.mean(scores[-100:]) > 200:
            print("üéâ AI ‡∏â‡∏•‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß! ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å")
            break

        if episode % 1000 == 0:
            model.save(f"ai_pong_{episode}.h5")
            np.savetxt("scores_log.txt", scores, fmt="%d")
            print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà Episode {episode}")

        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

        print(f"Episode {episode}: Score = {total_reward}, Epsilon = {epsilon:.4f}")

if __name__ == "__main__":
    train_dqn(episodes=5000)
    model.save("ai_pong_final.h5")
