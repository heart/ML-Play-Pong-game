import game
import tensorflow as tf
import numpy as np
import random
import time
import os
from collections import deque
import matplotlib.pyplot as plt

# ‡∏õ‡∏¥‡∏î Log ‡∏Ç‡∏≠‡∏á TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel('ERROR')

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ GPU ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("‚úÖ ‡πÉ‡∏ä‡πâ GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Training!")
else:
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU, ‡πÉ‡∏ä‡πâ CPU ‡πÅ‡∏ó‡∏ô")

# ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á RL
gamma = 0.95  # Discount Factor
epsilon = 1.0  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Exploration
epsilon_min = 0.05  # ‡∏Ñ‡πà‡∏≤ Epsilon ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
epsilon_decay = lambda ep: max(epsilon_min, epsilon * np.exp(-0.001 * ep))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏î epsilon
tau = 0.01  # Soft update ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Model ‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞ Target Model
learning_rate = 0.001
batch_size = 32
max_memory_size = 10000
memory = deque(maxlen=max_memory_size)
scores = []
num_frames = 4

state_size = (game.sceneSize, game.sceneSize, num_frames * 2)  # *2 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ speed channel
action_size = 3  # 3 Actions: ‡∏ã‡πâ‡∏≤‡∏¢, ‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà, ‡∏Ç‡∏ß‡∏≤

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Dueling DQN Model
def build_dueling_dqn():
    input_layer = tf.keras.layers.Input(shape=state_size)
    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input_layer)
    x = tf.keras.layers.MaxPooling2D(2,2)(x)
    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D(2,2)(x)
    x = tf.keras.layers.Flatten()(x)

    # ‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô State Value ‡πÅ‡∏•‡∏∞ Advantage
    state_value = tf.keras.layers.Dense(128, activation='relu')(x)
    state_value = tf.keras.layers.Dense(1, activation='linear')(state_value)

    advantage = tf.keras.layers.Dense(128, activation='relu')(x)
    advantage = tf.keras.layers.Dense(action_size, activation='linear')(advantage)

    # ‡∏£‡∏ß‡∏° State Value ‡πÅ‡∏•‡∏∞ Advantage ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    q_values = tf.keras.layers.Lambda(lambda a: a[0] + (a[1] - tf.keras.backend.mean(a[1], axis=1, keepdims=True)))([state_value, advantage])
    
    model = tf.keras.models.Model(inputs=input_layer, outputs=q_values)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')
    return model

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞ Target Network
model = build_dueling_dqn()
target_model = build_dueling_dqn()
target_model.set_weights(model.get_weights())

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Stack ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ü‡∏£‡∏°‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
frame_stack = deque(maxlen=num_frames)

def reset_stacked_state():
    """‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï Frame Stack ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Å‡∏°‡πÉ‡∏´‡∏°‡πà"""
    global frame_stack
    frame_stack.clear()

def get_stacked_state():
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ state ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏ü‡∏£‡∏°‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏•‡∏π‡∏Å"""
    state, ballXSpeed, ballYSpeed = game.get_state()
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° channel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
    state = np.array(state)
    direction_channel = np.zeros_like(state)
    direction_channel[0, 0] = ballXSpeed  # -1 = ‡∏ã‡πâ‡∏≤‡∏¢, 1 = ‡∏Ç‡∏ß‡∏≤
    direction_channel[0, 1] = ballYSpeed  # -1 = ‡∏Ç‡∏∂‡πâ‡∏ô, 1 = ‡∏•‡∏á
    
    state = np.stack([state, direction_channel], axis=-1)
    state = state.astype(np.float32) / 2  # Normalize

    if len(frame_stack) < num_frames:
        for _ in range(num_frames):
            frame_stack.append(state)

    frame_stack.append(state)
    stacked_state = np.concatenate(frame_stack, axis=-1)
    return stacked_state

def get_action(state, episode):
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Epsilon-Greedy Policy"""
    if np.random.rand() <= epsilon_decay(episode):
        return random.choice([0, 1, 2])  # ‡∏™‡∏∏‡πà‡∏° Action
    return np.argmax(model.predict(state.reshape(1, *state_size), verbose=0))

def train_dqn(episodes=5000):
    global epsilon
    best_model_score = -float('inf')

    for episode in range(episodes):
        reset_stacked_state()
        state = get_stacked_state()
        done = False
        total_reward = 0
        hits = 0  # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡πÇ‡∏î‡∏ô

        while not done:
            action = get_action(state, episode)
            if action == 0:
                game.move_left()
            elif action == 2:
                game.move_right()

            hit, game_over = game.tick()
            next_state = get_stacked_state()

            # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Reward Function
            reward = 10 if hit else -100 if game_over else 1
            total_reward += reward

            memory.append((state, action, reward, next_state, game_over))
            state = next_state

            game.render()
            print(f"Episode {episode}: Score = {total_reward}, Hits = {hits}, Epsilon = {epsilon_decay(episode):.4f}")
            
            if game_over:
                game.reset()
                reset_stacked_state()
                break

        scores.append(total_reward)
        avg_score = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        if avg_score > best_model_score:
            best_model_score = avg_score
            model.save("ai_pong_best.h5")
            print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Score: {avg_score:.2f})")

        print(f"Episode {episode}: Score = {total_reward}, Hits = {hits}, Epsilon = {epsilon_decay(episode):.4f}")

if __name__ == "__main__":
    train_dqn(episodes=5000)
    model.save("ai_pong_final.h5")
